"""Threads Scraper (with total post count)

æ”¯æ´ï¼š
* å–®ä¸€å¸³è™Ÿæˆ–å¤šå¸³è™Ÿæ‰¹æ¬¡
* --json è¼¸å‡ºåŸå§‹ JSON
* é¡å¤–é¡¯ç¤ºæ¯å€‹å¸³è™Ÿçš„ **ç¸½è²¼æ–‡æ•¸**ï¼ˆè‹¥èƒ½å¾ JSON å–å¾—ï¼‰
"""

import asyncio
import json
from typing import List, Dict, Optional
import jmespath
from nested_lookup import nested_lookup
from parsel import Selector
from playwright.async_api import async_playwright
import logging
from datetime import datetime

# è¨­å®š logger
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class Threads_scraper:
    def __init__(self, search_choice: str = "like_count", acc: bool = False, username: Optional[List[str]] = None):
        try:
            with open('existID.json', 'r', encoding='utf-8') as f:
                self.seen = set(json.load(f))
                logger.info(f"å·²è¼‰å…¥ existID.jsonï¼Œå…±æœ‰ {len(self.seen)} ç¯‡å·²éæ¿¾")
        except (FileNotFoundError, json.JSONDecodeError):
            self.seen = set()
            logger.warning("æ‰¾ä¸åˆ° existID.json æˆ–æ ¼å¼éŒ¯èª¤ï¼Œåˆå§‹åŒ–ç‚ºç©ºé›†åˆ")

        self.url = "https://www.threads.net/@"
        self.QUERY_SELECTOR = "script[type='application/json'][data-sjs]"
        self.search_choice = search_choice
        self.acc = acc
        self.gclike = 0
        self.gcreply = 0
        self.lttext = 0
        self.image_retrieve = False
        self.username = username

    def filter_setting(self, gclike: int = 0, gcreply: int = 0, lttext: int = 0, image_retrieve: bool = False):
        self.gclike = max(0, gclike)
        self.gcreply = max(0, gcreply)
        self.lttext = max(0, lttext)
        self.image_retrieve = image_retrieve
        logger.info(f"è¨­å®šéæ¿¾æ¢ä»¶ï¼šlike â‰¥ {self.gclike}, reply â‰¥ {self.gcreply}, text â‰¤ {self.lttext}, åœ–ç‰‡ï¼š{self.image_retrieve}")

    def _parse_post(self, item: Dict) -> Dict:
        return jmespath.search(
            """
            {
            id: post.id,
            code: post.code,
            text: post.caption.text,
            like_count: post.like_count,
            reply_count: post.text_post_app_info.direct_reply_count,
            username: post.user.username,
            timestamp: post.taken_at
            }
            """,
            item,
        )

    def _sort_posts(self, posts: List[Dict], sort_key: str, ascending: bool = False) -> List[Dict]:
        return sorted(posts, key=lambda p: p.get(sort_key) or 0, reverse=not ascending)

    async def Top_crawl(self, batch: int = 3) -> List[Dict]:
        logger.info(f"é–‹å§‹çˆ¬èŸ²ï¼Œå…± {len(self.username)} å€‹å¸³è™Ÿï¼Œæ¯å€‹å– {batch} ç¯‡")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()
            usertmp = self.username.copy()
            posts: List[Dict] = []

            for idx, username in enumerate(self.username):
                url = self.url + username
                logger.info(f"[{idx+1}/{len(self.username)}] æ­£åœ¨æŠ“å–ï¼š{url}")
                try:
                    await page.goto(url, timeout=60000)
                    await page.wait_for_selector(self.QUERY_SELECTOR, state="attached", timeout=60000)
                except Exception as e:
                    logger.error(f"ç„¡æ³•è¼‰å…¥ {url}ï¼š{e}")
                    continue

                selector = Selector(await page.content())
                scripts = selector.css(f"{self.QUERY_SELECTOR}::text").getall()
                done = False
                pre = len(posts)

                for payload in scripts:
                    if '"thread_items"' not in payload:
                        continue

                    data = json.loads(payload)
                    for group in nested_lookup("thread_items", data):
                        for item in group:
                            parsed = self._parse_post(item)
                            if self.gclike and int(parsed.get("like_count", 0)) < self.gclike:
                                continue
                            if self.gcreply and int(parsed.get("reply_count", 0)) < self.gcreply:
                                continue
                            if self.lttext and len(parsed.get("text", "") or "") > self.lttext:
                                continue
                            if self.image_retrieve == parsed.get("media_urls"):
                                continue
                            if parsed["id"] not in self.seen:
                                posts.append(parsed)
                                if len(posts) >= (idx + 1) * batch:
                                    break
                        if len(posts) >= (idx + 1) * batch:
                            done = True
                            break
                    if done:
                        break

                if len(posts) == pre:
                    usertmp.remove(username)
                    logger.warning(f"å¸³è™Ÿ {username} æ²’æœ‰æŠ“åˆ°ç¬¦åˆæ¢ä»¶çš„è²¼æ–‡")

            self.username = usertmp
            posts = self._sort_posts(posts, self.search_choice, self.acc)
            logger.info(f"å…±æŠ“åˆ° {len(posts)} ç¯‡è²¼æ–‡")
            return posts

    def printPost(self, posts):
        for idx, post in enumerate(posts, 1):
            excerpt = (post["text"] or "").replace("\n", " ")[:150]
            url = f"https://www.threads.net/@{post['username']}/post/{post['code']}"
            logger.info(f"{idx}. {excerpt}â€¦\n   ğŸ‘ {post['like_count']}   ğŸ’¬ {post['reply_count']}   {url}")

    def printJosn(self, posts):
        out = {"post": posts}
        logger.info(json.dumps(out, ensure_ascii=False, indent=1))

    def getJosn(self, posts):
        cleaned_posts = []
        for post in posts:
            cleaned_post = {
                "id": post.get("id", ""),
                "username": post.get("username", ""),
                "text": (post.get("text", "") or "").replace("\n", " "),
                "like_count": post.get("like_count", 0),
                "reply_count": post.get("reply_count", 0),
                "timestamp": post.get("timestamp", 0),
            }
            cleaned_posts.append(cleaned_post)
        out = {"posts": cleaned_posts}
        return json.dumps(out, ensure_ascii=False, indent=1)

if __name__ == "__main__":
    logger.info("è¼‰å…¥ threadsUser è¨­å®š")
    with open('config/threadsUser.json', 'r', encoding='utf-8') as f:
        cfg = json.load(f)

    threads = Threads_scraper(username=["huang.weizhu"])
    threads.filter_setting(gclike=1)
    posts = asyncio.run(threads.Top_crawl(batch=10))
    p = json.loads(threads.getJosn(posts))
    logger.info("å®Œæˆçˆ¬èŸ²è¼¸å‡º")
    logger.info(json.dumps(p, ensure_ascii=False, indent=1))
